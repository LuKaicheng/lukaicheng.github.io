---
title: HDFS学习总结(一)基本原理
date: 2017-08-02 11:08:08
tags:
- Hadoop
- HDFS
---

## 引言

HDFS(*Hadoop Distribute File System*)是一个开源的分布式文件系统，源自于Google公司在2003年发表的[GFS论文](https://research.google.com/archive/gfs.html)，目前已经成为Hadoop生态圈的基石之一。它是一个高度容错性的系统，能检测和应对硬件故障，被设计可以部署在低成本的通用硬件上。它简化了文件的一致性模型，通过流式数据访问，提供了高吞吐量的数据访问，非常适合那些拥有大规模数据集的应用。

<!-- more -->

## 基本架构

HDFS整体架构如下所示(*参考官网提供的架构图片*)：

![HDFS整体架构](https://raw.githubusercontent.com/LuKaicheng/lukaicheng.github.io/hexo/source/images/hdfs/HDFS_Architecture.png)

从图中不难发现，HDFS采用了**主从**模式的架构设计，包含一个**NameNode**和多个**DataNode**：

- **NameNode**：名称节点作为主服务管理着整个文件系统的命名空间并接收处理客户端对文件的访问请求，它能执行打开、关闭、重命名文件和目录等操作，同时也决定数据块到数据节点的映射关系。它存储的元数据包括文件/目录名称及相对于其父目录的位置，文件和目录的所有权及权限，各个数据块的文件名，值得注意的是它并不存储每个数据块的位置。
- **DataNode**：数据节点能够接收来自文件系统客户端的读写请求，另外根据名称节点的指示将会执行数据块的创建、删除和复制行为。当然，最主要的是它负责存储实际的组成文件的数据块数据。

## 备份和机架感知

每个存储在HDFS上的文件都会被分割为多个数据块(***Block***)，默认情况下一个数据块大小为128MB(*可以通过配置**hdfs-size.xml**的**dfs.blocksize**属性改变*)。在上一节曾提到，名称节点并不会保存每个数据块的位置，这是因为名称节点除了会接收数据节点发出的心跳包(***Heartbeat***)之外，还包括接收数据节点发出的数据块报告(***BlockReport***)，后者包含了数据节点上所有的数据块信息。

由于HDFS被设计成高容错系统，为了防止节点故障，这些数据块都是有备份的(*默认配置的备份数量是3，当然应用程序也可以指定文件的备份数，这可以在文件创建的时候指定，也可以在创建之后改变*)。另外，对于备份副本的放置，HDFS引入了**机架感知**的概念，其目的是为了提升数据可靠性，可用性以及网络带宽利用率。假设备份数是3，这个策略可以这样理解：如果写入文件的客户端也是一个数据节点，那么第一个备份就会放在本机上否则会随机挑选一个数据节点；第二个副本会被存放在与第一个副本不同的远程机架的一个数据节点上；第三个副本会被存放在第二个备份的机架上的另一台数据节点上。

## 写文件流程分析

假设写入文件被划分成2个数据块，备份因子是3，且HDFS具备机架感知功能，先放图：

![HDFS读文件流程](https://raw.githubusercontent.com/LuKaicheng/lukaicheng.github.io/hexo/source/images/hdfs/HDFS_Write_Process.png)

现在结合上图大致描述一下整个流程：

一、当客户端希望向HDFS写入文件，一开始是先将文件数据流式读入到本地文件系统中的一个临时文件，当文件数据大小达到一个数据块大小时，客户端才联系名称节点。

二、名称节点收到请求之后，会在HDFS文件系统的层级结构中创建一个文件，然后把数据块的标识符和数据节点的位置信息发送给客户端：

```
Block1: DN1,DN3,DN4
Block2: DN5,DN8,DN7
```
三、客户端收到数据块信息之后，那么将开始向数据节点发送数据包：

1. 客户端首先尝试发送Block1到DN1，Block1会被划分成更小的数据包发送给DN1
2. 当DN1接收到第一个数据包并写入到本地存储之后，它会将数据包传送给DN3
3. 当DN3接收到数据包并将其写入本地存储之后，它会发送给第三个数据节点DN4
4. 最终，DN4会将数据包写入到本地存储，这样文件数据最终以管道的方式保存并备份
5. 当数据包保存成功之后，会反向沿着数据管道返回确认消息(ACK)
6. 当Block1发送成功之后，客户端继续发送Block2，过程类似

四、客户端收到数据块保存成功的确认，客户端会发送最终的确认信息给名称节点。最终当文件被关闭时，名称节点会执行一个提交操作，从而使得该文件在集群中为可见状态。

通过整个过程，我们可以意识到：

- 写1T文件，需要3T的存储，3T的网络流量带宽。
- 在整个写过程中，名称节点和数据节点通过HeartBeat保持通信，一旦发现数据节点故障，感知到文件数据没有备份完成，就会重新执行备份，把数据备份到状态良好的节点。

## 读文件流程分析

以上一节为基础，再放一张关于读文件的示意图：

![HDFS写文件流程](https://raw.githubusercontent.com/LuKaicheng/lukaicheng.github.io/hexo/source/images/hdfs/HDFS_Read_Process.png)

结合上图简要分析一下读文件的大致流程：

一、客户端会先向名称节点发送读文件请求。

二、名称节点收到请求之后，会返回组成文件的数据块列表及数据块的位置(包括备份数据块的位置)：  

```
Block1: DN1,DN3,DN4
Block2: DN5,DN8,DN7
```

三、客户端收到数据块信息之后，可以直接访问数据节点获取组成文件的各个数据块，首先从DN1节点获取Block1，然后从DN5节点获取Block5。如果访问的数据节点发生故障或者在计算数据块校验和的时候校验失败，那么会访问存放备份数据块的数据节点(*比如DN1发生问题，那么会从DN3获取Block1*)。

## 回收站机制

HDFS提供了一种回收站的机制，针对意外的删除行为，它给予了用户一次反悔的机会，主要的运行机制大概是这样的：当用户删除文件之后，首先会被移动到**/user/{username}/.Trash/Current/**目录，并且它的原始文件系统路径会被保留，经过一段可配置的时间(*通过属性**fs.trash.interval**配置*)之后，回收站检查进程会将**Current**目录根据当前时间重命名为**/user/{username}/.Trash/{current_timestamp}/**，并且查找过往检查点的**timestamp**目录，并删除过期的目录，只有到这一步，名称节点才会将文件从命名空间删除，并释放该文件相关的数据块，随后HDFS系统会显示增加了一些空闲空间。

## 局限性

通过上面的分析中不难看出，名称节点扮演着非常关键的角色，可以说整个HDFS都是围绕着它来运转。正因为如此，在hadoop1.x系统中，名称节点很容易成为单点故障，从而导致整个集群处于不可用状态。另外，由于名称节点维护了整个系统的命名空间，它要负责存储所有的元数据信息，即使HDFS针对的是大文件，随着集群规模和所存储文件的增加，命名节点的容量也将成为整个集群的瓶颈。因此，hadoop2.x在高可用和扩展性这两个方面提出了针对性的补充，这些就留待下一篇再进行总结。

## 参考

[深入理解Hadoop](https://book.douban.com/subject/26684358/)

[HDFS Architecture](https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html)

[Rack Awareness](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/RackAwareness.html)

[HDFS运行原理](http://www.cnblogs.com/laov/p/3434917.html)

[Configuring HDFS Trash](https://www.cloudera.com/documentation/enterprise/5-7-x/topics/cm_mc_config_trash.html)